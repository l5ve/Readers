{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import pymysql\n",
    "import re\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "# mysql db connect\n",
    "db = pymysql.connect(host='52.78.23.232', user='root', password='readers7', port=3306, db='readers', charset='utf8', cursorclass=pymysql.cursors.DictCursor)\n",
    "# browser = webdriver.Chrome('C:/Users/KSY/Documents/GitHub/Readers/crawling/chromedriver.exe')\n",
    "cursor1 = db.cursor()\n",
    "cursor2 = db.cursor()\n",
    "cursor3 = db.cursor()\n",
    "cursor4 = db.cursor()\n",
    "cursor5 = db.cursor()\n",
    "\n",
    "#웹 페이지를 열고 소스코드를 읽어오는 작업\n",
    "def readHtml(url):\n",
    "    # print(url)\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    html.close()\n",
    "    return soup\n",
    "\n",
    "# 연재 중 웹툰 크롤링\n",
    "\n",
    "sel_sql = \"select toon_id, is_end from toon_info where toon_id = %s\"\n",
    "ins_sql = \"insert into toon_info(toon_id, toon_name, toon_url, toon_desc, toon_site, wrt_name, is_end, toon_thumb_url)\\\n",
    "           values (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "genre_sql = \"insert into toon_genre(toon_id, genre_name)\\\n",
    "             values (%s, %s)\"\n",
    "weekday_sql = \"insert ignore into toon_weekday(toon_id, toon_weekday)\\\n",
    "               values (%s, %s)\"\n",
    "upd_sql = \"update toon_info set is_end = %s where toon_id = %s and is_end = %s\"\n",
    "\n",
    "soup = readHtml(\"http://comic.naver.com/webtoon/weekday.nhn\")\n",
    "\n",
    "#요일별 웹툰영역 추출하기\n",
    "data1_list=soup.findAll('div',{'class':'thumb'})\n",
    "# pprint(data1_list)\n",
    "\n",
    "#전체 웹툰 리스트\n",
    "a_list = []\n",
    "for data1 in data1_list:\n",
    "    a_list.extend(data1.findAll('a'))\n",
    "\n",
    "# 모든 요일 링크 추출\n",
    "for a in a_list:\n",
    "    src = a['href']\n",
    "    \n",
    "    url = urlparse(src)\n",
    "    toon_id = 'naver_' + str(parse_qs(url.query)['titleId']).replace(\"['\", '').replace(\"']\", '') # ID 생성\n",
    "    \n",
    "    toon_url = 'https://comic.naver.com' + src + '&page=' # 웹툰 페이지 추출\n",
    "    \n",
    "    soup = readHtml(toon_url+'1') # 네이버 웹툰 첫페이지\n",
    "    \n",
    "    # 요일 추출\n",
    "    if 'weekday=mon' in toon_url:\n",
    "        weekday = 'mon'\n",
    "    if 'weekday=tue' in toon_url:\n",
    "        weekday = 'tue'\n",
    "    if 'weekday=wed' in toon_url:\n",
    "        weekday = 'wed'\n",
    "    if 'weekday=thu' in toon_url:\n",
    "        weekday = 'thu'\n",
    "    if 'weekday=fri' in toon_url:\n",
    "        weekday = 'fri'\n",
    "    if 'weekday=sat' in toon_url:\n",
    "        weekday = 'sat'\n",
    "    if 'weekday=sun' in toon_url:\n",
    "        weekday = 'sun'\n",
    "        \n",
    "    cursor1.execute(weekday_sql, (toon_id, weekday))\n",
    "    \n",
    "    detail = soup.find('div', {'class':'detail'})\n",
    "    toon_name = detail.find('h2')\n",
    "    \n",
    "    wrt_nm = str(toon_name.find('span', {'class':'wrt_nm'}))\n",
    "    wrt_name = re.sub('<.+?>', '', wrt_nm, 0).strip() # 작가 추출\n",
    "    \n",
    "    toon_name = str(toon_name).replace('\\t', '').replace('컷툰', '')\n",
    "    toon_name = re.sub('<.+?>', '', toon_name, 0).strip()\n",
    "    terminator = toon_name.index('\\n')\n",
    "    toon_name = toon_name[:terminator].strip('컷') # 제목 추출\n",
    "    \n",
    "    # 장르 추출\n",
    "    genres = str(detail.find('span', {'class':'genre'}))\n",
    "    genres = re.sub('<.+?>', '', genres, 0).strip()\n",
    "    genre_list = genres.split(', ')\n",
    "\n",
    "    desc = str(detail.find('p')).replace('<br/>', '\\n')\n",
    "    toon_desc = re.sub('<.+?>', '', desc, 0).strip() # 설명 추출\n",
    "    \n",
    "    toon_thumb_url = soup.find('img')['src'] # 썸네일 주소 추출\n",
    "    \n",
    "    # toon_info\n",
    "    cursor2.execute(sel_sql, toon_id)\n",
    "    result = cursor2.fetchall()\n",
    "    \n",
    "    res_o = (item for item in result if (toon_id == item['toon_id'] and 'O' == item['is_end']))\n",
    "    res_o_list = []\n",
    "    for res_o_item in res_o:\n",
    "        res_o_list.append(res_o_item)\n",
    "    \n",
    "    res_x = (item for item in result if (toon_id == item['toon_id'] and 'X' == item['is_end']))\n",
    "    res_x_list = []\n",
    "    for res_x_item in res_x:\n",
    "        res_x_list.append(res_x_item)\n",
    "        \n",
    "    # 완결이었다가 재연재한 웹툰 DB 변경\n",
    "    if res_o_list:\n",
    "        cursor5.execute(upd_sql, ('X', toon_id, 'O'))\n",
    "        db.commit()\n",
    "    \n",
    "    elif not res_x_list:\n",
    "        cursor3.execute(ins_sql, (toon_id, toon_name, toon_url, toon_desc, 'naver', wrt_name, 'X', toon_thumb_url))\n",
    "        for genre_name in genre_list:\n",
    "            cursor4.execute(genre_sql, (toon_id, genre_name))\n",
    "            db.commit()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
