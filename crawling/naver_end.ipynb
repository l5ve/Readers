{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist\n",
      "already exist\n",
      "already exist\n",
      "already exist\n",
      "already exist\n",
      "already exist\n",
      "already exist\n",
      "already exist\n",
      "already exist\n",
      "already exist\n",
      "finish commit\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import pymysql\n",
    "import re\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "# mysql db connect\n",
    "db = pymysql.connect(host='52.78.23.232', user='root', password='readers7', port=3306, db='readers', charset='utf8', cursorclass=pymysql.cursors.DictCursor)\n",
    "# browser = webdriver.Chrome('C:/Users/KSY/Documents/GitHub/Readers/crawling/chromedriver.exe')\n",
    "cursor1 = db.cursor()\n",
    "cursor2 = db.cursor()\n",
    "cursor3 = db.cursor()\n",
    "cursor4 = db.cursor()\n",
    "\n",
    "#웹 페이지를 열고 소스코드를 읽어오는 작업\n",
    "def readHtml(url):\n",
    "    # print(url)\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    html.close()\n",
    "    return soup\n",
    "\n",
    "## 완결 웹툰 크롤링 ##\n",
    "\n",
    "sel_sql = \"select toon_id, is_end from toon_info where toon_id = %s\"\n",
    "ins_sql = \"insert into toon_info(toon_id, toon_name, toon_url, toon_desc, toon_site, wrt_name, is_end, toon_thumb_url)\\\n",
    "           values (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "genre_sql = \"insert ignore into toon_genre(toon_id, genre_name)\\\n",
    "             values (%s, %s)\"\n",
    "upd_sql = \"update toon_info set is_end = %s where toon_id = %s and is_end = %s\"\n",
    "\n",
    "soup = readHtml(\"https://comic.naver.com/webtoon/finish.nhn\")\n",
    "\n",
    "#요일별 웹툰영역 추출하기\n",
    "data1_list=soup.findAll('div',{'class':'thumb'})\n",
    "# pprint(data1_list)\n",
    "\n",
    "#전체 웹툰 리스트\n",
    "a_list = []\n",
    "for data1 in data1_list:\n",
    "    a_list.extend(data1.findAll('a'))\n",
    "# pprint(a_list)\n",
    "\n",
    "# 웹툰 정보 추출\n",
    "count = 0\n",
    "for a in a_list:\n",
    "    src = a['href']\n",
    "    url = 'https://comic.naver.com' + src + '&page=' # 웹툰 페이지 추출\n",
    "    \n",
    "    url = urlparse(src)\n",
    "    toon_id = 'naver_' + str(parse_qs(url.query)['titleId']).replace(\"['\", '').replace(\"']\", '') # ID 생성\n",
    "\n",
    "    toon_url = 'https://comic.naver.com' + src + '&page=' # 웹툰 페이지 추출\n",
    "    \n",
    "    soup = readHtml(toon_url+'1') # 네이버 웹툰 첫페이지\n",
    "    \n",
    "    detail = soup.find('div', {'class':'detail'})\n",
    "    toon_name = detail.find('h2')\n",
    "    \n",
    "    wrt_nm = str(toon_name.find('span', {'class':'wrt_nm'}))\n",
    "    wrt_name = re.sub('<.+?>', '', wrt_nm, 0).strip() # 작가 추출\n",
    "    \n",
    "    toon_name = str(toon_name).replace('\\t', '').replace('컷툰', '')\n",
    "    toon_name = re.sub('<.+?>', '', toon_name, 0).strip()\n",
    "    terminator = toon_name.index('\\n')\n",
    "    toon_name = toon_name[:terminator].strip('컷') # 제목 추출\n",
    "    \n",
    "    # 장르 추출\n",
    "    genres = str(detail.find('span', {'class':'genre'}))\n",
    "    genres = re.sub('<.+?>', '', genres, 0).strip()\n",
    "    genre_list = genres.split(', ')\n",
    "\n",
    "    desc = str(detail.find('p')).replace('<br/>', '\\n')\n",
    "    toon_desc = re.sub('<.+?>', '', desc, 0).strip() # 설명 추출\n",
    "    \n",
    "    toon_thumb_url = soup.find('img')['src'] # 썸네일 주소 추출\n",
    "    \n",
    "    # toon_info\n",
    "    cursor1.execute(sel_sql, toon_id)\n",
    "    result = cursor1.fetchall()\n",
    "\n",
    "    res_o = (item for item in result if (toon_id == item['toon_id'] and 'O' == item['is_end']))\n",
    "    res_o_list = []\n",
    "    for res_o_item in res_o:\n",
    "        res_o_list.append(res_o_item)\n",
    "        \n",
    "    res_x = (item for item in result if (toon_id == item['toon_id'] and 'X' == item['is_end']))\n",
    "    res_x_list = []\n",
    "    for res_x_item in res_x:\n",
    "        res_x_list.append(res_x_item)\n",
    "    \n",
    "    # 이미 완결 웹툰 DB에 들어있는지 확인\n",
    "    if res_o_list:\n",
    "        print('already exist')\n",
    "        count += 1\n",
    "\n",
    "    # 연재 중이었다가 완결이 난 웹툰의 완결 여부 수정\n",
    "    elif res_x_list:\n",
    "        cursor4.execute(upd_sql, ('O', toon_id, 'X'))\n",
    "    \n",
    "    else:\n",
    "        cursor2.execute(ins_sql, (toon_id, toon_name, toon_url, toon_desc, 'naver', wrt_name, 'O', toon_thumb_url))\n",
    "        print('execute cur2')\n",
    "        for genre_name in genre_list:\n",
    "            cursor3.execute(genre_sql, (toon_id, genre_name))\n",
    "            print('execute cur3')\n",
    "            db.commit()\n",
    "            \n",
    "    if (count==10):\n",
    "        print('finish commit')\n",
    "        break\n",
    "        \n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
