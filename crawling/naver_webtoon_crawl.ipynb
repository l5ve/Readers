{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import pymysql\n",
    "import re\n",
    "\n",
    "db = pymysql.connect(host='52.78.23.232', user='root', password='readers7', port=3306, db='readers', charset='utf8')\n",
    "# browser = webdriver.Chrome('C:/Users/KSY/Documents/GitHub/Readers/crawling/chromedriver.exe')\n",
    "cursor = db.cursor()\n",
    "sql = \"insert into toon_info(toon_name, toon_url, description, toon_site, toon_weekday, wrt_nm, toon_genre, is_end)\\\n",
    "       values (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "\n",
    "#웹 페이지를 열고 소스코드를 읽어오는 작업\n",
    "def readHtml(url):\n",
    "    # print(url)\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    html.close()\n",
    "    return soup\n",
    "\n",
    "soup = readHtml(\"http://comic.naver.com/webtoon/weekday.nhn\")\n",
    "\n",
    "#요일별 웹툰영역 추출하기\n",
    "data1_list=soup.findAll('div',{'class':'thumb'})\n",
    "# pprint(data1_list)\n",
    "\n",
    "#전체 웹툰 리스트\n",
    "a_list = []\n",
    "for data1 in data1_list:\n",
    "    a_list.extend(data1.findAll('a'))\n",
    "# pprint(a_list)\n",
    "\n",
    "# 모든 요일 링크 추출\n",
    "for a in a_list:\n",
    "    src = a['href']\n",
    "    url = 'https://comic.naver.com' + src + '&page=' # 웹툰 페이지 추출\n",
    "    \n",
    "    soup = readHtml(url+'1') # 네이버 웹툰 첫페이지\n",
    "    \n",
    "    # 요일 추출\n",
    "    if 'weekday=mon' in url:\n",
    "        weekday = 'mon'\n",
    "    if 'weekday=tue' in url:\n",
    "        weekday = 'tue'\n",
    "    if 'weekday=wed' in url:\n",
    "        weekday = 'wed'\n",
    "    if 'weekday=thu' in url:\n",
    "        weekday = 'thu'\n",
    "    if 'weekday=fri' in url:\n",
    "        weekday = 'fri'\n",
    "    if 'weekday=sat' in url:\n",
    "        weekday = 'sat'\n",
    "    if 'weekday=sun' in url:\n",
    "        weekday = 'sun'\n",
    "    \n",
    "    detail = soup.find('div', {'class':'detail'})\n",
    "    title = detail.find('h2')\n",
    "    \n",
    "    wrt_nm = str(title.find('span', {'class':'wrt_nm'}))\n",
    "    wrt_nm = re.sub('<.+?>', '', wrt_nm, 0).strip() # 작가 추출\n",
    "    \n",
    "    title = str(title).replace('\\t', '').replace('컷툰', '')\n",
    "    title = re.sub('<.+?>', '', title, 0).strip()\n",
    "    terminator = title.index('\\n')\n",
    "    title = title[:terminator].strip('컷') # 제목 추출\n",
    "    \n",
    "    genre = str(detail.find('span', {'class':'genre'}))\n",
    "    genre = re.sub('<.+?>', '', genre, 0).strip() # 장르 추출\n",
    "\n",
    "    desc = str(detail.find('p')).replace('<br/>', '\\n')\n",
    "    desc = re.sub('<.+?>', '', desc, 0).strip() # 설명 추출\n",
    "    \n",
    "    thumb = soup.find('img')['src'] # 썸네일 주소 추출\n",
    "    \n",
    "    cursor.execute(sql, (title, url, desc, 'naver', weekday, wrt_nm, genre, 'X'))\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import pymysql\n",
    "import re\n",
    "\n",
    "db = pymysql.connect(host='52.78.23.232', user='root', password='readers7', port=3306, db='readers', charset='utf8')\n",
    "# browser = webdriver.Chrome('C:/Users/KSY/Documents/GitHub/Readers/crawling/chromedriver.exe')\n",
    "cursor = db.cursor()\n",
    "sql = \"insert into toon_info(toon_name, toon_url, description, toon_site, toon_weekday, wrt_nm, toon_genre, is_end)\\\n",
    "       values (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "\n",
    "#웹 페이지를 열고 소스코드를 읽어오는 작업\n",
    "def readHtml(url):\n",
    "    # print(url)\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    html.close()\n",
    "    return soup\n",
    "\n",
    "soup = readHtml(\"https://comic.naver.com/webtoon/finish.nhn\")\n",
    "\n",
    "#요일별 웹툰영역 추출하기\n",
    "data1_list=soup.findAll('div',{'class':'thumb'})\n",
    "# pprint(data1_list)\n",
    "\n",
    "#전체 웹툰 리스트\n",
    "a_list = []\n",
    "for data1 in data1_list:\n",
    "    a_list.extend(data1.findAll('a'))\n",
    "# pprint(a_list)\n",
    "\n",
    "# 웹툰 정보 추출\n",
    "for a in a_list:\n",
    "    src = a['href']\n",
    "    url = 'https://comic.naver.com' + src + '&page=' # 웹툰 페이지 추출\n",
    "    \n",
    "    soup = readHtml(url+'1') # 네이버 웹툰 첫페이지\n",
    "    \n",
    "    detail = soup.find('div', {'class':'detail'})\n",
    "    title = detail.find('h2')\n",
    "    \n",
    "    wrt_nm = str(title.find('span', {'class':'wrt_nm'}))\n",
    "    wrt_nm = re.sub('<.+?>', '', wrt_nm, 0).strip() # 작가 추출\n",
    "    \n",
    "    title = str(title).replace('\\t', '').replace('컷툰', '')\n",
    "    title = re.sub('<.+?>', '', title, 0).strip()\n",
    "    terminator = title.index('\\n')\n",
    "    title = title[:terminator].strip('컷') # 제목 추출\n",
    "    \n",
    "    genre = str(detail.find('span', {'class':'genre'}))\n",
    "    genre = re.sub('<.+?>', '', genre, 0).strip() # 장르 추출\n",
    "\n",
    "    desc = str(detail.find('p')).replace('<br/>', '\\n')\n",
    "    desc = re.sub('<.+?>', '', desc, 0).strip() # 설명 추출\n",
    "    \n",
    "    thumb = soup.find('img')['src'] # 썸네일 주소 추출\n",
    "    \n",
    "    cursor.execute(sql, (title, url, desc, 'naver', None, wrt_nm, genre, 'O'))\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
